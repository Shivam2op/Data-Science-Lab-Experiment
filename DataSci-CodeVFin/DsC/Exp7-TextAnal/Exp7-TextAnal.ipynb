{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d86897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448d7dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AAKASH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AAKASH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\AAKASH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AAKASH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data (required for stopwords)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f450669b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'science', 'is', 'a', 'field', 'of', 'study', 'that', 'uses', 'modern', 'tools', 'and', 'techniques', 'such', 'as', 'machine', 'learning', 'algorithms', 'to', 'unify', 'statistics', ',', 'data', 'analysis', ',', 'informatics', 'and', 'their', 'related', 'methods', 'in', 'order', 'to', 'understand', 'and', 'analyze', 'actual', 'phenomena', 'with', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "document = \"Data science is a field of study that uses modern tools and techniques such as machine learning algorithms to unify statistics, data analysis, informatics and their related methods in order to understand and analyze actual phenomena with data.\"\n",
    "tokens = word_tokenize(document.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a01bd61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 'NNS'), ('science', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('of', 'IN'), ('study', 'NN'), ('that', 'WDT'), ('uses', 'VBZ'), ('modern', 'JJ'), ('tools', 'NNS'), ('and', 'CC'), ('techniques', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('machine', 'NN'), ('learning', 'VBG'), ('algorithms', 'NNS'), ('to', 'TO'), ('unify', 'VB'), ('statistics', 'NNS'), (',', ','), ('data', 'NNS'), ('analysis', 'NN'), (',', ','), ('informatics', 'NNS'), ('and', 'CC'), ('their', 'PRP$'), ('related', 'JJ'), ('methods', 'NNS'), ('in', 'IN'), ('order', 'NN'), ('to', 'TO'), ('understand', 'VB'), ('and', 'CC'), ('analyze', 'VB'), ('actual', 'JJ'), ('phenomena', 'NN'), ('with', 'IN'), ('data', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52bb57c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'science', 'field', 'study', 'uses', 'modern', 'tools', 'techniques', 'machine', 'learning', 'algorithms', 'unify', 'statistics', ',', 'data', 'analysis', ',', 'informatics', 'related', 'methods', 'order', 'understand', 'analyze', 'actual', 'phenomena', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0727c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'scienc', 'field', 'studi', 'use', 'modern', 'tool', 'techniqu', 'machin', 'learn', 'algorithm', 'unifi', 'statist', ',', 'data', 'analysi', ',', 'informat', 'relat', 'method', 'order', 'understand', 'analyz', 'actual', 'phenomena', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6591fc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'science', 'field', 'study', 'us', 'modern', 'tool', 'technique', 'machine', 'learning', 'algorithm', 'unify', 'statistic', ',', 'data', 'analysis', ',', 'informatics', 'related', 'method', 'order', 'understand', 'analyze', 'actual', 'phenomenon', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58fffbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 31)\t0.14002800840280097\n",
      "  (0, 17)\t0.14002800840280097\n",
      "  (0, 0)\t0.14002800840280097\n",
      "  (0, 3)\t0.14002800840280097\n",
      "  (0, 28)\t0.14002800840280097\n",
      "  (0, 16)\t0.14002800840280097\n",
      "  (0, 8)\t0.14002800840280097\n",
      "  (0, 13)\t0.14002800840280097\n",
      "  (0, 18)\t0.14002800840280097\n",
      "  (0, 25)\t0.14002800840280097\n",
      "  (0, 9)\t0.14002800840280097\n",
      "  (0, 2)\t0.14002800840280097\n",
      "  (0, 20)\t0.14002800840280097\n",
      "  (0, 29)\t0.14002800840280097\n",
      "  (0, 26)\t0.28005601680560194\n",
      "  (0, 1)\t0.14002800840280097\n",
      "  (0, 11)\t0.14002800840280097\n",
      "  (0, 12)\t0.14002800840280097\n",
      "  (0, 5)\t0.14002800840280097\n",
      "  (0, 22)\t0.14002800840280097\n",
      "  (0, 23)\t0.14002800840280097\n",
      "  (0, 4)\t0.42008402520840293\n",
      "  (0, 27)\t0.14002800840280097\n",
      "  (0, 14)\t0.14002800840280097\n",
      "  (0, 30)\t0.14002800840280097\n",
      "  (0, 24)\t0.14002800840280097\n",
      "  (0, 21)\t0.14002800840280097\n",
      "  (0, 15)\t0.14002800840280097\n",
      "  (0, 7)\t0.14002800840280097\n",
      "  (0, 10)\t0.14002800840280097\n",
      "  (0, 19)\t0.14002800840280097\n",
      "  (0, 6)\t0.42008402520840293\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform([document])\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f64b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
