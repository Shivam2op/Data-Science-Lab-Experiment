{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a30554a",
   "metadata": {},
   "source": [
    "# 7) Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of documents by calculating Term Frequency and Inverse DocumentFrequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99df07c",
   "metadata": {},
   "source": [
    "# In this example, we first define the sample document. Then, we perform the following preprocessing steps:\n",
    "\n",
    "1) Tokenization: Break the document into individual words or tokens using word_tokenize() function.\n",
    "2) POS Tagging: Assign a part-of-speech tag to each token using pos_tag() function.\n",
    "3) Stop Words Removal: Remove stop words from the tokens using a set of stop words from the NLTK corpus.\n",
    "4) Stemming: Apply stemming to reduce words to their base or root form using PorterStemmer().\n",
    "5) Lemmatization: Apply lemmatization to transform words to their base form using WordNetLemmatizer().\n",
    "\n",
    "Finally, we calculate the Term Frequency (TF) using FreqDist() from NLTK and the Inverse Document Frequency (IDF) using TfidfVectorizer() from scikit-learn.\n",
    "\n",
    "Adjust the code as needed for your specific requirements and incorporate your own document data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ac44a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/pratik/anaconda3/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /Users/pratik/anaconda3/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/pratik/anaconda3/lib/python3.10/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in /Users/pratik/anaconda3/lib/python3.10/site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in /Users/pratik/anaconda3/lib/python3.10/site-packages (from nltk) (8.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afde67a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['This', 'is', 'a', 'sample', 'document', '.', 'It', 'contains', 'multiple', 'sentences', 'and', 'words', '.', 'We', 'will', 'apply', 'various', 'preprocessing', 'techniques', 'on', 'this', 'document', '.']\n",
      "\n",
      "\n",
      "POS Tags: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('document', 'NN'), ('.', '.'), ('It', 'PRP'), ('contains', 'VBZ'), ('multiple', 'JJ'), ('sentences', 'NNS'), ('and', 'CC'), ('words', 'NNS'), ('.', '.'), ('We', 'PRP'), ('will', 'MD'), ('apply', 'VB'), ('various', 'JJ'), ('preprocessing', 'VBG'), ('techniques', 'NNS'), ('on', 'IN'), ('this', 'DT'), ('document', 'NN'), ('.', '.')]\n",
      "\n",
      "\n",
      "Filtered Tokens (Stop Words Removal): ['sample', 'document', '.', 'contains', 'multiple', 'sentences', 'words', '.', 'apply', 'various', 'preprocessing', 'techniques', 'document', '.']\n",
      "\n",
      "\n",
      "Stemmed Tokens: ['sampl', 'document', '.', 'contain', 'multipl', 'sentenc', 'word', '.', 'appli', 'variou', 'preprocess', 'techniqu', 'document', '.']\n",
      "\n",
      "\n",
      "Lemmatized Tokens: ['sample', 'document', '.', 'contains', 'multiple', 'sentence', 'word', '.', 'apply', 'various', 'preprocessing', 'technique', 'document', '.']\n",
      "\n",
      "\n",
      "Term Frequency (TF): <FreqDist with 11 samples and 14 outcomes>\n",
      "\n",
      "\n",
      "Inverse Document Frequency (IDF): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample Document\n",
    "document = \"\"\"\n",
    "This is a sample document. It contains multiple sentences and words. \n",
    "We will apply various preprocessing techniques on this document.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(document)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.casefold() not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "# Term Frequency (TF)\n",
    "tf = FreqDist(lemmatized_tokens)\n",
    "\n",
    "# Inverse Document Frequency (IDF)\n",
    "corpus = [document]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "idf = vectorizer.idf_\n",
    "\n",
    "# Print the results\n",
    "print(\"Tokens:\", tokens)\n",
    "print()\n",
    "print()\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "print()\n",
    "print()\n",
    "print(\"Filtered Tokens (Stop Words Removal):\", filtered_tokens)\n",
    "print()\n",
    "print()\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print()\n",
    "print()\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
    "print()\n",
    "print()\n",
    "print(\"Term Frequency (TF):\", tf)\n",
    "print()\n",
    "print()\n",
    "print(\"Inverse Document Frequency (IDF):\", idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca3c0c",
   "metadata": {},
   "source": [
    "# In this result, you can see the different stages of document preprocessing:\n",
    "\n",
    "1) Tokenization: The document is split into individual tokens.\n",
    "2) POS Tagging: Each token is assigned a part-of-speech tag.\n",
    "3) Filtered Tokens: Stop words are removed, resulting in a reduced set of tokens.\n",
    "4) Stemmed Tokens: The filtered tokens are stemmed using PorterStemmer.\n",
    "5) Lemmatized Tokens: The filtered tokens are lemmatized using WordNetLemmatizer.\n",
    "\n",
    "Additionally, the Term Frequency (TF) is calculated using FreqDist(), which provides the frequency of each token in the lemmatized tokens. The Inverse Document Frequency (IDF) is also calculated, which in this case is a single value of 1.0 since there is only one document in the corpus.\n",
    "\n",
    "Please note that the provided sample document is small, resulting in limited diversity and repetition in the tokens. In a real-world scenario with a larger corpus, the TF and IDF values would provide more meaningful insights.\n",
    "\n",
    "Adjust the code and incorporate your own document data to see the results for your specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4ba07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
